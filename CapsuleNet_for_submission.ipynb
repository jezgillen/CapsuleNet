{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CapsuleNet for submission.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VCwte8nwMfM",
        "colab_type": "text"
      },
      "source": [
        "#Building a capsule network\n",
        "COMP9417\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V66ng3UvC1Bu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03811111-b9a0-4861-be55-b2059f9261ea"
      },
      "source": [
        "##!/usr/bin/python3\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.datasets import mnist\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "tf.compat.v1.set_random_seed(0)\n",
        "\n",
        "\n",
        "\n",
        "def squash(x):\n",
        "        # [batch, ..., capsules, atoms] -> same shape, but if you norm along the atoms axis\n",
        "        # Below is safe approximation to the norm, prevents death by nan\n",
        "        norms = tf.linalg.norm(x+1e-7,axis=-1,keep_dims=True) \n",
        "        squash = norms**2/(1+norms**2)*(x/norms)\n",
        "        return squash\n",
        "\n",
        "class Model_Base():\n",
        "    \"\"\"\n",
        "    A neural net base class, containing functions for training\n",
        "    \"\"\"\n",
        "    folder = '/content/drive/My Drive/Colab Notebooks'\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.checkpoint = None\n",
        "\n",
        "\n",
        "    def save(self, model_name):\n",
        "        if(not hasattr(self,'checkpoint')):\n",
        "            location = self.folder+'/ckpt_'+model_name+'/'\n",
        "            #create dict of variables\n",
        "            vars_list = [(str(i),var) for i,var in enumerate(self.trainable_variables)]\n",
        "            vars_dict = dict(vars_list)\n",
        "            self.checkpoint = tf.train.Checkpoint(opt=self.opt, **vars_dict)\n",
        "            self.manager = tf.train.CheckpointManager(self.checkpoint, location,max_to_keep=3)\n",
        "\n",
        "        self.manager.save()\n",
        "        print(f\"Checkpoint {self.checkpoint.save_counter.numpy()} saved\")\n",
        "\n",
        "\n",
        "    def load(self, model_name):\n",
        "        if(not hasattr(self,'checkpoint')):\n",
        "            location = self.folder+'/ckpt_'+model_name+'/'\n",
        "            #create dict of variables\n",
        "            vars_list = [(str(i),var) for i,var in enumerate(self.trainable_variables)]\n",
        "            vars_dict = dict(vars_list)\n",
        "            self.checkpoint = tf.train.Checkpoint(opt=self.opt, **vars_dict)\n",
        "            self.manager = tf.train.CheckpointManager(self.checkpoint, location,max_to_keep=3)\n",
        "        self.checkpoint.restore(self.manager.latest_checkpoint)\n",
        "        if(self.manager.latest_checkpoint):\n",
        "            print(f\"Checkpoint restored from {self.manager.latest_checkpoint}\")\n",
        "        else:\n",
        "            print(\"Initialized model from scratch\")\n",
        "\n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        if(len(self.layers) == 0):\n",
        "            raise NotImplementedError(\n",
        "                    \"No layers in 'self.layers', you should have put them there in __init__\"\n",
        "                    )\n",
        "        trainable_variables = []\n",
        "        for l in self.layers:\n",
        "            trainable_variables += l.trainable_variables\n",
        "        return trainable_variables\n",
        "      \n",
        "    @property\n",
        "    def num_parameters(self):\n",
        "        trainable_variables = self.trainable_variables\n",
        "        total = 0\n",
        "        for var in self.trainable_variables:\n",
        "            product = 1\n",
        "            for axis in var.shape:\n",
        "                product *= axis\n",
        "            total += product if product != 1 else 0\n",
        "        return total\n",
        "\n",
        "    def train(self, X, Y, num_epochs=10, batch_size=128):\n",
        "        \"\"\"\n",
        "        Batch SGD\n",
        "        Yields control back every epoch, returning the average loss across batches\n",
        "        \"\"\"\n",
        "        assert(hasattr(self,'opt')) # you haven't set up an optimizer\n",
        "\n",
        "        N = tf.shape(X)[0]\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss_history = []\n",
        "            for batch in range(tf.math.ceil(N/batch_size)):\n",
        "                start = batch*batch_size\n",
        "                end = min((batch+1)*batch_size, N)\n",
        "\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss = self.loss(X[start:end],Y[start:end])\n",
        "\n",
        "                loss_history.append(tf.reduce_sum(loss).numpy())\n",
        "\n",
        "                weights = self.trainable_variables\n",
        "                gradients = tape.gradient(loss, weights)\n",
        "                self.opt.apply_gradients(zip(gradients, weights))\n",
        "\n",
        "            yield np.mean(loss_history)\n",
        "            \n",
        "    def train_with_datagen(self, X, Y, generator, num_epochs=10, batch_size=128):\n",
        "        \"\"\"\n",
        "        Batch SGD\n",
        "        Yields control back every epoch, returning the average loss across batches\n",
        "        \"\"\"\n",
        "        assert(hasattr(self,'opt')) # you haven't set up an optimizer\n",
        "\n",
        "        N = tf.shape(X)[0]\n",
        "        X = np.expand_dims(X,-1)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss_history = []\n",
        "            batch = 0\n",
        "            for X_batch, Y_batch in generator.flow(X,Y,batch_size):\n",
        "                batch += 1\n",
        "                with tf.GradientTape() as tape:\n",
        "                    loss = self.loss(np.squeeze(X_batch),Y_batch)\n",
        "\n",
        "                loss_history.append(tf.reduce_sum(loss).numpy())\n",
        "\n",
        "                weights = self.trainable_variables\n",
        "                gradients = tape.gradient(loss, weights)\n",
        "                self.opt.apply_gradients(zip(gradients, weights))\n",
        "                \n",
        "                if batch > N/batch_size:\n",
        "                    break\n",
        "\n",
        "            yield np.mean(loss_history)\n",
        "\n",
        "    def loss(self, X, Y):\n",
        "        raise NotImplementedError()\n",
        "    def predict(self, X):\n",
        "        raise NotImplementedError()\n",
        "    def batch_predict(self, X, batch_size=256):\n",
        "        ''' for doing predictions without killing GPU memory '''\n",
        "        N = tf.shape(X)[0]\n",
        "        prediction_history = []\n",
        "        for batch in range(tf.math.ceil(N/batch_size)):\n",
        "            start = batch*batch_size\n",
        "            end = min((batch+1)*batch_size, N)\n",
        "            predictions = self.predict(X[start:end])\n",
        "            prediction_history.append(predictions)\n",
        "        return tf.concat(prediction_history, axis=0)\n",
        "\n",
        "\n",
        "class Layer():\n",
        "    def build(self, *args, **kwargs):\n",
        "        # Must create trainable_variables attribute\n",
        "        raise NotImplementedError()\n",
        "    def call(self, *args, **kwargs):\n",
        "        raise NotImplementedError()\n",
        "    def __call__(self, *args, **kwargs):    \n",
        "        try:\n",
        "            self.output_shape\n",
        "        except AttributeError:\n",
        "            self.build(*args, **kwargs)\n",
        "        return self.call(*args,**kwargs)\n",
        "\n",
        "class Conv2D(Layer):\n",
        "    ''' \n",
        "    A wrapper for tf.nn.conv2d. Takes a tensor of shape \n",
        "    [batch, height, width, channels] and outputs a tensor of shape\n",
        "    [batch, out_height, out_width, out_channels]\n",
        "    '''\n",
        "    def build(self, x, filter_shape=[9,9], output_channels=5, strides=1):\n",
        "        input_shape = x.shape\n",
        "        self.strides=strides\n",
        "        weights_shape=[filter_shape[0],filter_shape[1],input_shape[-1],output_channels]\n",
        "        #  self.w = tf.Variable(tf.random.uniform(),dtype=tf.float32)\n",
        "        #  self.w = (self.w - 0.5)/100\n",
        "        self.w = tf.Variable(\n",
        "                tf.random.truncated_normal(weights_shape)/100.,\n",
        "                dtype=tf.float32\n",
        "                )\n",
        "\n",
        "        #set batch size to 1\n",
        "        input_shape = list(input_shape)\n",
        "        self.output_shape = [-1, \n",
        "                            ((input_shape[1]-filter_shape[0])//strides)+1,\n",
        "                            ((input_shape[2]-filter_shape[1])//strides)+1,\n",
        "                            output_channels,\n",
        "                            ]\n",
        "        self.trainable_variables = [self.w]\n",
        "\n",
        "\n",
        "    def call(self, x, filter_shape=[9,9], output_channels=5, strides=1):\n",
        "        return tf.nn.conv2d(x, self.w, self.strides, padding=\"VALID\")\n",
        "\n",
        "class Dense(Layer):\n",
        "    def build(self, x, output_size=10):\n",
        "        input_shape = x.shape\n",
        "        assert(len(input_shape) == 2)\n",
        "        self.w = tf.Variable(\n",
        "                tf.random.truncated_normal((input_shape[-1],output_size))/100.,\n",
        "                dtype=tf.float32\n",
        "                )\n",
        "        self.b = tf.Variable(tf.random.truncated_normal([output_size])/100.,dtype=tf.float32)\n",
        "\n",
        "        self.output_shape = (-1,output_size)\n",
        "        self.trainable_variables = [self.w, self.b]\n",
        "\n",
        "    def call(self, x, output_size=10):\n",
        "        return x@self.w + self.b\n",
        "      \n",
        "\n",
        "\n",
        "class DenseCaps(Layer):\n",
        "    '''\n",
        "    Example usage:\n",
        "    d = denseCaps(x, out_caps=8, out_atoms=5, routing_iterations=2)\n",
        "\n",
        "    Takes a tensor x of shape [batch, in_capsules, in_atoms],\n",
        "    where in_atoms is the sive of the vector that makes up each capsule in the previous \n",
        "    layer.\n",
        "    Outputs a tensor d of shape [batch, out_capsules, out_atoms]\n",
        "    Contains weights of shape [in_capsules, in_atoms, out_capsules, out_dims]\n",
        "    '''\n",
        "    def build(self, x, out_caps=5, out_atoms=4,routing_iterations=3):\n",
        "        input_shape = x.shape\n",
        "        # initialise weights\n",
        "        weight_shape = (input_shape[1],input_shape[2],out_caps,out_atoms)\n",
        "        self.w = tf.Variable(\n",
        "                tf.random.truncated_normal(weight_shape)/100.,\n",
        "                dtype=tf.float32)\n",
        "\n",
        "        self.routing_iterations = routing_iterations\n",
        "        self.in_caps = input_shape[1]\n",
        "        self.out_caps = out_caps\n",
        "        \n",
        "        # set trainable_variables and output_shape\n",
        "        self.trainable_variables = [self.w]\n",
        "        self.output_shape = (-1,out_caps,out_atoms)\n",
        "\n",
        "\n",
        "    def call(self, x, out_caps=5, out_atoms=4,routing_iterations=3):\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        # initialise routing weights b\n",
        "        b = tf.Variable(tf.zeros((batch_size,self.in_caps,self.out_caps,1)),dtype=tf.float32)\n",
        "\n",
        "        for r in range(self.routing_iterations):\n",
        "            # get softmax over b, called c\n",
        "            c = tf.nn.softmax(b,axis=-2)\n",
        "            # get votes of shape [batch, in_caps, out_caps, out_atoms], called $u$ in paper\n",
        "            votes = tf.einsum('abc,bcde->abde', x, self.w)\n",
        "            # sum (axis 2) over votes*expand_dims(c)\n",
        "            s = tf.reduce_sum(votes*c,axis=1)\n",
        "            # squash, currently of shape [batch, out_caps, out_atoms]\n",
        "            squashed = squash(s)\n",
        "            if(r < self.routing_iterations-1):\n",
        "                # dot with votes\n",
        "                reshaped_squashed = tf.expand_dims(squashed,1)\n",
        "                a = tf.reduce_sum(reshaped_squashed*votes,axis=-1)\n",
        "                # add to b, updating routing weights\n",
        "                tf.compat.v1.assign_add(b, tf.expand_dims(a,-1))\n",
        "\n",
        "        return squashed\n",
        "\n",
        "    \n",
        "\n",
        "class CapsNet(Model_Base):\n",
        "    '''\n",
        "    Takes an MNIST batch of shape [batch, 28, 28] and \n",
        "    outputs softmax prediction of shape [batch, 10]\n",
        "    '''\n",
        "    def __init__(self, learning_rate=0.001):\n",
        "        # initialise layers\n",
        "        self.opt = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        self.conv1 = Conv2D()\n",
        "        self.conv2 = Conv2D()\n",
        "        self.capsules = DenseCaps()\n",
        "\n",
        "        self.dense1 = Dense()\n",
        "        self.dense2 = Dense()\n",
        "        self.dense3 = Dense()\n",
        "\n",
        "        self.layers = [self.conv1, self.conv2, self.capsules,\n",
        "                        self.dense1, self.dense2, self.dense3]\n",
        "\n",
        "        x = np.zeros((1,28,28),dtype=np.float32)\n",
        "        y = np.zeros((1,10),dtype=np.float32)\n",
        "        self.loss(x,y)\n",
        "\n",
        "\n",
        "    def model(self, X, training=False):\n",
        "        X = tf.expand_dims(X,-1)\n",
        "        # X is now [batch, h, w, channels]\n",
        "        h = tf.nn.relu(self.conv1(X, (9,9), 256))\n",
        "        # TODO should the below be a squash function?\n",
        "        h2 = (self.conv2(h, filter_shape=(9,9), output_channels=32*8,strides=2))\n",
        "        s = self.conv2.output_shape\n",
        "        h2 = tf.reshape(h2, (-1,s[1]*s[2]*32,8))\n",
        "        # shape is [batch, h*w*caps, atoms]\n",
        "        h2 = squash(h2)\n",
        "        h3 = self.capsules(h2, out_caps=10, out_atoms=16)\n",
        "        # shape is [batch, 10, 16]\n",
        "        return h3\n",
        "\n",
        "    def reconstruction(self, h3, X, Y):\n",
        "        #mask all except correct capsule\n",
        "        # h3 [batch, 10, 16]\n",
        "        # Y [batch, 10]\n",
        "        Y = tf.expand_dims(Y, -1)\n",
        "        # Y [batch, 10, 1]\n",
        "        masked = h3*Y\n",
        "        masked = tf.reshape(masked, (-1, 10*16))\n",
        "        h1 = self.dense1(masked, 512)\n",
        "        h2 = self.dense2(h1, 1024)\n",
        "        prediction = self.dense3(h2, 784)\n",
        "        # shape [batch, 784]\n",
        "        X = tf.reshape(X, (-1, 784))\n",
        "        return tf.reduce_sum((X-prediction)**2, axis=-1)\n",
        "        \n",
        "    def predict(self, X):\n",
        "        h3 = self.model(X)\n",
        "        digit_cap_lengths = tf.linalg.norm(h3, axis=-1)\n",
        "        return digit_cap_lengths\n",
        "\n",
        "    def loss(self, X, Y, training=True):\n",
        "        h3 = self.model(X, training=training)\n",
        "        digit_cap_lengths = tf.linalg.norm(h3+1e-7, axis=-1)\n",
        "        reconstruction_loss = self.reconstruction(h3, X, Y)\n",
        "        # TESTING here\n",
        "        margin_loss = Y*tf.nn.relu(0.9-digit_cap_lengths)**2 + \\\n",
        "                0.5*(1-Y)*tf.nn.relu(digit_cap_lengths-0.1)**2\n",
        "        margin_loss = tf.reduce_sum(margin_loss, axis=-1)\n",
        "        total_loss = margin_loss + 0.0005*reconstruction_loss\n",
        "        return total_loss\n",
        "        \n",
        "\n",
        "\n",
        "def main():\n",
        "    (X, Y), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "    Y_test = tf.one_hot(Y_test, 10, 1.0, 0.0)\n",
        "    X_test = tf.cast(X_test,tf.float32)/255.\n",
        "    Y = tf.one_hot(Y, 10, 1.0, 0.0)\n",
        "    X = tf.cast(X,tf.float32)/255.\n",
        "    \n",
        "    # Split validation set off of training set\n",
        "    # Training size:    50,000\n",
        "    # Validation size:  10,000\n",
        "    # Test size:        10,000\n",
        "    val_size = len(Y_test)\n",
        "    Y_val = Y[0:val_size,:]\n",
        "    X_val = X[0:val_size:,:,:]\n",
        "    X = X[val_size:,:,:]\n",
        "    Y = Y[val_size:,:]\n",
        "    \n",
        "    datagen = ImageDataGenerator(width_shift_range=3,\n",
        "                                           height_shift_range=3)  # augment with two pixel shifts\n",
        "    model_name = 'caps_augmented_test9'\n",
        "    model = CapsNet(learning_rate=0.001)\n",
        "    print(f\"{model.num_parameters} parameters in model\")\n",
        "    #model.load(model_name) # Load saved model if it exists\n",
        "    \n",
        "    for epoch,loss in enumerate(model.train_with_datagen(X,Y,datagen,num_epochs=10,batch_size=64)):\n",
        "        print(f\"Epoch {epoch}\")\n",
        "        print(f\"Training Loss: {loss}\")\n",
        "\n",
        "        testPredictions = model.batch_predict(X_val)\n",
        "        accuracy = get_accuracy(testPredictions, Y_val)\n",
        "        print(f\"Validation Accuracy: {accuracy*100:.2F}%\")\n",
        "        #if(epoch%5 == 4):\n",
        "        #    model.save(model_name)\n",
        "        print()\n",
        "    # Print validation confusion matrix\n",
        "    # get_accuracy(testPredictions, Y_val, True)\n",
        "    \n",
        "    # For testing only, after algorithm is finalised\n",
        "    testPredictions = model.batch_predict(X_test)\n",
        "    accuracy = get_accuracy(testPredictions, Y_test, confusion=True)\n",
        "    print(f\"Final testing accuracy is: {accuracy*100:.2F}%\")\n",
        "    \n",
        "    \n",
        "def get_accuracy(predictions, y, confusion=False):\n",
        "    y_true = np.argmax(y.numpy(),axis=-1)\n",
        "    y_pred = np.argmax(predictions,axis=-1)\n",
        "    accuracy = np.sum(np.equal(y_pred, y_true))/len(y_true)\n",
        "    if confusion:\n",
        "        print(\"Confusion matrix:\")\n",
        "        print(metrics.confusion_matrix(y_true, y_pred))\n",
        "        print()\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "8215056 parameters in model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0810 19:10:09.761942 139695137187712 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Training Loss: 6.588250160217285\n",
            "Validation Accuracy: 98.57%\n",
            "\n",
            "Epoch 1\n",
            "Training Loss: 2.8038201332092285\n",
            "Validation Accuracy: 98.75%\n",
            "\n",
            "Epoch 2\n",
            "Training Loss: 2.3082289695739746\n",
            "Validation Accuracy: 98.80%\n",
            "\n",
            "Epoch 3\n",
            "Training Loss: 2.0195586681365967\n",
            "Validation Accuracy: 99.04%\n",
            "\n",
            "Epoch 4\n",
            "Training Loss: 1.798672080039978\n",
            "Validation Accuracy: 99.01%\n",
            "\n",
            "Epoch 5\n",
            "Training Loss: 1.6777493953704834\n",
            "Validation Accuracy: 99.07%\n",
            "\n",
            "Epoch 6\n",
            "Training Loss: 1.5643140077590942\n",
            "Validation Accuracy: 99.13%\n",
            "\n",
            "Epoch 7\n",
            "Training Loss: 1.4734833240509033\n",
            "Validation Accuracy: 99.14%\n",
            "\n",
            "Epoch 8\n",
            "Training Loss: 1.413800597190857\n",
            "Validation Accuracy: 99.32%\n",
            "\n",
            "Epoch 9\n",
            "Training Loss: 1.3358128070831299\n",
            "Validation Accuracy: 99.21%\n",
            "\n",
            "Confusion matrix:\n",
            "[[ 979    0    0    0    0    0    0    1    0    0]\n",
            " [   0 1131    1    0    0    0    1    0    0    2]\n",
            " [   1    0 1030    0    0    0    0    1    0    0]\n",
            " [   0    0    1 1004    0    4    0    1    0    0]\n",
            " [   0    0    0    0  971    0    5    0    0    6]\n",
            " [   0    0    0    3    0  885    1    1    0    2]\n",
            " [   2    4    0    0    0    1  950    0    1    0]\n",
            " [   0    2    7    0    0    0    0 1016    0    3]\n",
            " [   0    0    0    1    0    1    1    3  966    2]\n",
            " [   0    0    0    0    4    0    0    2    0 1003]]\n",
            "\n",
            "Final testing accuracy is: 99.35%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}